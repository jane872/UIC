# Iris Classification Model API (Milestone 1)
This project implements an **Iris flower classification model** using Scikit-learn's Random Forest algorithm, encapsulated as a FastAPI service. It supports deployment to **GCP Cloud Run (containerized)** and **GCP Cloud Function (serverless)**, with a comparison of the two deployment approaches.


## Project Structure
```
├── train_model.py       # Trains the model and saves it as model.pkl
├── main.py              # Core FastAPI service code
├── requirements.txt     # Dependencies for the FastAPI service
├── Dockerfile           # Container configuration for Cloud Run
├── cloud_function/      # Files for GCP Cloud Function
│   ├── main.py          # Cloud Function entry code
│   └── requirements.txt # Dependencies for Cloud Function
├── model.pkl            # Trained model file (generated by train_model.py)
└── README.md            # Project documentation
```


## Core Features
1. Exposes a prediction API for Iris classification (input: 4 flower features; output: predicted class + confidence).
2. FastAPI service includes **request schema validation** and a health check endpoint.
3. Supports deployment to GCP Cloud Run (stateful container) and Cloud Function (stateless serverless).
4. Enables comparison of cold start latency, lifecycle, and other characteristics between the two deployment methods.


## Prerequisites
1. Environment: Python 3.9+
2. GCP Setup (for cloud deployment):
   - A GCP account and project
   - [gcloud CLI](https://cloud.google.com/sdk/docs/install) installed and initialized
   - Enabled APIs: Artifact Registry, Cloud Run, Cloud Functions, Cloud Storage


## Step 1: Local Environment Setup & Run
### 1. Clone/Download the Project
Place project files in a local directory and navigate to the root folder.

### 2. Create & Activate a Virtual Environment
```bash
# Create virtual environment
python -m venv venv

# Activate (Windows)
venv\Scripts\activate
# Activate (Mac/Linux)
source venv/bin/activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Train the Model & Generate `model.pkl`
```bash
python train_model.py
```
A `model.pkl` file (trained Random Forest model) will be generated in the root directory.

### 5. Start the Local FastAPI Service
```bash
uvicorn main:app --reload
```

### 6. Test the Local Service
Test via **either** method:
1. **Swagger UI**: Visit `http://localhost:8000/docs`, click "Try it out" for the `/predict` endpoint, input test data, and execute.
2. **curl Commands**:
   ```bash
   # Test prediction endpoint
   curl -X POST "http://localhost:8000/predict" \
   -H "Content-Type: application/json" \
   -d '{"sepal_length": 5.1, "sepal_width": 3.5, "petal_length": 1.4, "petal_width": 0.2}'

   # Test health check endpoint
   curl -X GET "http://localhost:8000/health"
   ```
**Expected Results**:
- Prediction: `{"prediction":0,"confidence":1.0}`
- Health check: `{"status":"healthy","model_loaded":true}`


## Step 2: Deploy to GCP Cloud Run
### 1. Configure GCP Project & Region
```bash
# Set your GCP project ID
gcloud config set project <YOUR_GCP_PROJECT_ID>

# Set region (e.g., asia-east1)
gcloud config set run/region asia-east1
```

### 2. Create an Artifact Registry Repository
```bash
gcloud artifacts repositories create ml-model-repo \
--repository-format=docker \
--location=asia-east1 \
--description="Docker repo for Iris FastAPI model"
```

### 3. Build the Docker Image
```bash
# Replace <YOUR_GCP_PROJECT_ID> with your project ID
docker build -t asia-east1-docker.pkg.dev/<YOUR_GCP_PROJECT_ID>/ml-model-repo/iris-fastapi:v1 .
```

### 4. Authenticate Docker with GCP
```bash
gcloud auth configure-docker asia-east1-docker.pkg.dev
```

### 5. Push the Image to Artifact Registry
```bash
docker push asia-east1-docker.pkg.dev/<YOUR_GCP_PROJECT_ID>/ml-model-repo/iris-fastapi:v1
```

### 6. Deploy to Cloud Run
```bash
gcloud run deploy iris-fastapi-service \
--image asia-east1-docker.pkg.dev/<YOUR_GCP_PROJECT_ID>/ml-model-repo/iris-fastapi:v1 \
--platform managed \
--allow-unauthenticated
```

### 7. Validate Cloud Run Service
After deployment, a **HTTPS URL** will be displayed. Test with:
```bash
curl -X POST "<YOUR_CLOUD_RUN_URL>/predict" \
-H "Content-Type: application/json" \
-d '{"sepal_length": 5.1, "sepal_width": 3.5, "petal_length": 1.4, "petal_width": 0.2}'
```
**Expected Result**: Same as local service (prediction + confidence).


## Step 3: Deploy to GCP Cloud Function
### 1. Prepare GCS Bucket & Upload Model
1. In the GCP Console, create a Cloud Storage bucket (name: `<YOUR_BUCKET_NAME>`, region matching your project).
2. Upload `model.pkl` (from the project root) to this bucket.

### 2. Configure Cloud Function Code
Navigate to the `cloud_function` directory, open `main.py`, and update the bucket name:
```python
# Replace with your GCS bucket name
bucket_name = "<YOUR_BUCKET_NAME>"
```

### 3. Deploy the Cloud Function
```bash
# Navigate to cloud_function directory
cd cloud_function

# Deploy (replace <YOUR_GCP_PROJECT_ID> with your project ID)
gcloud functions deploy iris-classification-function \
--runtime python311 \
--trigger-http \
--allow-unauthenticated \
--entry-point predict \
--project <YOUR_GCP_PROJECT_ID> \
--region asia-east1
```

### 4. Validate Cloud Function
After deployment, a **HTTPS URL** will be displayed. Test with:
```bash
curl -X POST "<YOUR_CLOUD_FUNCTION_URL>" \
-H "Content-Type: application/json" \
-d '{"sepal_length": 5.1, "sepal_width": 3.5, "petal_length": 1.4, "petal_width": 0.2}'
```
**Expected Result**: `{"prediction":0,"confidence":1.0}`


## Comparison of GCP Deployment Methods
| Dimension               | GCP Cloud Run                  | GCP Cloud Function            |
|-------------------------|--------------------------------|-------------------------------|
| Deployment Method       | Containerized (Docker image)   | Serverless (code upload)      |
| Statefulness            | Stateful (container persists)  | Stateless (initialized per call) |
| Cold Start Latency      | Moderate (100ms–2s)            | Higher (1–5s, model-dependent)|
| Resource Control        | Custom CPU/memory configuration| Fixed resources, auto-scaling |
| Model Loading           | Loaded on container start (persists) | Loaded on first call (reused if warm) |
| Use Case                | High-concurrency, low-latency  | Low-concurrency, on-demand tasks |


## Troubleshooting
1. `ModuleNotFoundError` when running `train_model.py`: Ensure the virtual environment is activated and dependencies are installed via `requirements.txt`.
2. Cloud Run image push failure: Verify GCP project ID/region and that the Artifact Registry API is enabled.
3. Cloud Function model download failure: Confirm the bucket name is correct and the function has read access to the GCS bucket.
4. Permission errors for cloud services: Ensure `--allow-unauthenticated` was added during deployment (for testing).


## Summary
1. This project encapsulates an Iris classification model as a service and deploys it via two GCP methods—reusable for other ML models.
2. Use virtual environments for local dependency isolation; enable GCP APIs and configure permissions for cloud deployment.
3. Choose Cloud Run for low-latency, high-concurrency workloads; choose Cloud Function for cost-effective, on-demand tasks.
